{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS NN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K # Tensorflow\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, fnmatch\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def read_data_set(data_set):\n",
    "    list_ = []\n",
    "    pin_id = ['1','2','3','4','5','6','7','8'] # Electrode(Pin) id names\n",
    "    # Find Measurement(x).csv files in folder =======================================\n",
    "    pattern = '*.csv'\n",
    "    directory_path = 'C:\\\\Users\\\\Alex\\\\Desktop\\\\DS DNN basic functions\\\\10_300'\n",
    "    directory = os.listdir('C:\\\\Users\\\\Alex\\\\Desktop\\\\DS DNN basic functions\\\\10_300')# Import directory's file names\n",
    "    sum_files = 0\n",
    "    files = [] # Create the list of Measurement files\n",
    "    for file in directory:\n",
    "        if fnmatch.fnmatch(file, pattern): # Check if True -> \"file.pattern\"\n",
    "            data = pd.read_csv(directory_path+\"\\\\\"+file, delimiter=';', header=None)\n",
    "            data.index = pin_id # Chenge index pin id number\n",
    "            list_.append(data) # Put all data in a list for later use\n",
    "            files.append(file) # \n",
    "            sum_files += 1 \n",
    "    data_set = pd.concat(list_)# Make the list a dataframe\n",
    "    data_set.columns = ['50khZ','100khZ','150khZ','200khZ','250khZ','300khZ','350khZ','400khZ','450khZ','500khZ']\n",
    "    data_set.index.name = 'Pin'\n",
    "    print (\"Size of DataFrame:\", data_set.shape)\n",
    "    print ('Path: ',directory_path)\n",
    "    print ('Imported files for Training :', files)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create Training set from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_data_handling(train_data_set): \n",
    "    train_set_1 = data_set.iloc[:,0:5].copy() # Split data_set in half\n",
    "    train_set_1.columns=['x1', 'x2', 'x3', 'x4', 'x5'] # Change name of columns for the concat\n",
    "    train_set_1['y'] = 0 # Give an outpot label to each set(row)\n",
    "    \n",
    "    train_set_2 = data_set.iloc[:,5:10].copy()\n",
    "    train_set_2.columns=['x1', 'x2', 'x3', 'x4', 'x5']\n",
    "    train_set_2['y'] = 1\n",
    "    \n",
    "    train_data_set = pd.concat([train_set_1, train_set_2])\n",
    "    \n",
    "    #X: Input, Y: Output\n",
    "    X, Y = train_data_set.drop('y', axis=1).values , train_data_set['y'].values\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "    \n",
    "    return train_data_set, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_NN(x,l1,l2,l3,a1,a2,a3,ep,i):\n",
    "    # Model, Layers\n",
    "    model = Sequential()\n",
    "    model.add(Dense(l1, input_dim=x, activation=a1))\n",
    "    model.add(Dense(l2, activation=a2))\n",
    "    model.add(Dense(l3, activation=a3))\n",
    "    # Model Train\n",
    "    accuracy_train, accuracy_test = [], []\n",
    "    for iterations in range(i):\n",
    "        # Model initialization\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']);\n",
    "        model.fit(X_train, Y_train, epochs=ep, shuffle=True, verbose=0)\n",
    "        # Model test\n",
    "        score_train = model.evaluate(X_train, Y_train)\n",
    "        score_test = model.evaluate(X_test, Y_test)\n",
    "        accuracy_train.append(score_train[1]*100) # Keep scores for visual\n",
    "        accuracy_test.append(score_test[1]*100) # Keep scores for visual\n",
    "        \n",
    "    #print(\"\\n%s: %.2f%%\" % ('Train Accuracy', score_train[1]*100))\n",
    "    #print(\"\\n%s: %.2f%%\" % ('Test Accuracy', score_test[1]*100))\n",
    "    #print(accuracy_train)\n",
    "    #print(accuracy_test)\n",
    "    #model.summary()\n",
    "    return score_train, score_test, accuracy_train, accuracy_test, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def visual(): \n",
    "    val_train = pd.DataFrame(accuracy_train)\n",
    "    val_train['score_type'] = 'Train accuracy'\n",
    "    val_test = pd.DataFrame(accuracy_test)\n",
    "    val_test['score_type'] = 'Test accuracy'\n",
    "    val = pd.concat([val_train, val_test])\n",
    "    val.columns = ['Accuracy%', 'Error type']\n",
    "    val.index.name = 'Iteration'\n",
    "    \n",
    "    fg1 = sns.pointplot(x=val.index, y='Accuracy%', data=val, hue='Error type')\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step1: Read & Manipulate data\n",
    "data_set = pd.DataFrame()\n",
    "data_set = read_data_set(data_set)\n",
    "#display(data_set) # only jupyter notebook\n",
    "\n",
    "# Step2: Data Handling for NN Model\n",
    "# If sample is 50-250kHz, Output(y)=0, If sample is 300-500kHz, Output(y)=1\n",
    "train_data_set = pd.DataFrame()\n",
    "train_data_set, X_train, X_test, Y_train, Y_test = train_data_handling(train_data_set)\n",
    "\n",
    "# Step3: Train NN Model \n",
    "# x:Input dimesnsions, l:Layer size, a:Activation function, ep:Epochs, i:Training iterations\n",
    "score_train, score_test, accuracy_train, accuracy_test, model = train_NN(\n",
    "    x=5,l1=4,l2=4,l3=1,a1='relu',a2='relu',a3='sigmoid',ep=40, i=25)\n",
    "K.clear_session() # Clear model's parameters\n",
    "\n",
    "# Step4: Visual NN's Train and Test accuracy\n",
    "visual() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
